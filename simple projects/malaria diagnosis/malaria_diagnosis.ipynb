{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tawheed-tariq/Machine-learning-course/blob/main/simple%20projects/malaria%20diagnosis/malaria_diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsTUmH6ATCOu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Conv2D, MaxPool2D, Flatten, BatchNormalization, Input, Layer, Dropout, RandomFlip, RandomRotation, Resizing, Rescaling\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError, FalsePositives, FalseNegatives, TrueNegatives, TruePositives, Precision, Recall, AUC, BinaryAccuracy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import L2, L1\n",
        "from tensorflow.image import flip_left_right\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk_i645RTIPL"
      },
      "outputs": [],
      "source": [
        "dataset, dataset_info = tfds.load('malaria', with_info = True, as_supervised = True, shuffle_files= True, split = ['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be63fTLkTt_h"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIGLLUl_iM_P"
      },
      "outputs": [],
      "source": [
        "def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n",
        "  DATASET_SIZE = len(dataset)\n",
        "\n",
        "  train_dataset = dataset.take(int(TRAIN_RATIO * DATASET_SIZE))\n",
        "  val_test_dataset = dataset.skip(int(TRAIN_RATIO * DATASET_SIZE))\n",
        "\n",
        "  val_dataset = val_test_dataset.take(int(VAL_RATIO * DATASET_SIZE))\n",
        "\n",
        "  test_dataset = val_test_dataset.skip(int(VAL_RATIO * DATASET_SIZE))\n",
        "  return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd-UcXYDhkQA"
      },
      "outputs": [],
      "source": [
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "\n",
        "train_dataset , val_dataset, test_dataset = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n",
        "\n",
        "# print(list(train_dataset.take(1).as_numpy_iterator()), list(val_dataset.take(1).as_numpy_iterator()), list(test_dataset.take(1).as_numpy_iterator()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCbLOutnnoKB"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igWj3fKxU4Lz"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i , (image, label) in enumerate(train_dataset.take(16)):\n",
        "  ax = plt.subplot(4, 4, i+1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(dataset_info.features['label'].int2str(label))\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmtspfgYnw63"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Argumentation"
      ],
      "metadata": {
        "id": "ECn0Cog1Bhtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(orignal, argumented):\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(orignal)\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(argumented)"
      ],
      "metadata": {
        "id": "LeEgdZWjBkqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orignal_image, label = next(iter(train_dataset))"
      ],
      "metadata": {
        "id": "eAam7ifCB-Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "argumented_image = tf.image.central_crop(orignal_image, 0.8)"
      ],
      "metadata": {
        "id": "4QjNF-5UOUI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(orignal_image, argumented_image)"
      ],
      "metadata": {
        "id": "Nu8A7INyB9td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUFQhXVabcJA"
      },
      "outputs": [],
      "source": [
        "IM_SIZE = 224\n",
        "def resize_rescale(image, label):\n",
        "  return tf.image.resize(image, (IM_SIZE, IM_SIZE))/ 255.0, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resize_rescale_layers = tf.keras.Sequential([\n",
        "    Resizing(IM_SIZE, IM_SIZE),\n",
        "    Rescaling(1.0/255.0)\n",
        "])"
      ],
      "metadata": {
        "id": "NCpBgl2WJS0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data argumentation using tf.image\n",
        "def argument(image, label):\n",
        "    image , label = resize_rescale(image, label)\n",
        "\n",
        "    image = tf.image.rot90(image, k = 1)\n",
        "    image = tf.image.flip_left_right(image)\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "AZRPnATzPY9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotNienty(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def call(self, image):\n",
        "    return tf.image.rot90(image, k = 1)"
      ],
      "metadata": {
        "id": "7oROYO7dNHay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data argumentation using tf.keras.layers\n",
        "argument_layers = tf.keras.Sequential([\n",
        "    RotNienty(),\n",
        "    RandomFlip(mode = 'horizontal')\n",
        "])\n",
        "\n",
        "def argument_layer(image, label):\n",
        "    return argument_layers(resize_rescale_layers(image) , training = True), label"
      ],
      "metadata": {
        "id": "RVtUwfSTXZ-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "FM3SmVC5aCn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y0IFPTKtXch"
      },
      "outputs": [],
      "source": [
        "# test_dataset = test_dataset.map(resize_rescale_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So1ne88EZAg3"
      },
      "outputs": [],
      "source": [
        "# for image, label in train_dataset.take(1):\n",
        "#   print(image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G2wBlQ9ZMtL"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size= 8, reshuffle_each_iteration= True)\n",
        "   # .map(argument_layer)\n",
        "    .batch(1)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .shuffle(buffer_size= 8, reshuffle_each_iteration= True)\n",
        "    #.map(resize_rescale_layers)\n",
        "    .batch(1)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MIxUp Data argumentation"
      ],
      "metadata": {
        "id": "HzPyosxXaFGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1 = train_dataset.shuffle(buffer_size= 8, reshuffle_each_iteration= True).map(resize_rescale)\n",
        "train_dataset_2 = train_dataset.shuffle(buffer_size= 8, reshuffle_each_iteration= True).map(resize_rescale)\n",
        "\n",
        "mixed_dataset = tf.data.Dataset.zip((train_dataset_1, train_dataset_2))"
      ],
      "metadata": {
        "id": "ujlggItVd5aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def mixup(train_dataset_1, train_dataset_2):\n",
        "    (image_1, label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n",
        "\n",
        "    lamda = tfp.distributions.Beta(0.1, 0.1)\n",
        "    lamda = lamda.sample(1)[0]\n",
        "\n",
        "    image = lamda * image_1 + (1 - lamda) * image_2\n",
        "    label = lamda * tf.cast(label_1, dtype=float32) + (1 - lamda) * tf.cast(label_2, dtype= float32)\n",
        "\n",
        "    return image , label"
      ],
      "metadata": {
        "id": "SegVylZlaIyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = (\n",
        "    mixed_dataset\n",
        "    .shuffle(buffer_size= 8, reshuffle_each_iteration= True)\n",
        "   .map(mixup)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "zFsUs4qie86n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .map(resize_rescale)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "LwWGkAyogFGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzPQ3aVV_RIF"
      },
      "source": [
        "# Model Creation and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JATj7_EDbyO9"
      },
      "outputs": [],
      "source": [
        "dropout_rate = 0.2\n",
        "regularizing_rate = 0.01\n",
        "model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape=(None, None, 3)),\n",
        "    resize_rescale_layers,\n",
        "    argument_layers,\n",
        "\n",
        "    Conv2D(filters = 6, kernel_size = 3, padding = 'valid', strides = 1, activation = 'relu', kernel_regularizer = L2(regularizing_rate)),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size = 2, strides = 2),\n",
        "    Dropout(rate = dropout_rate),\n",
        "\n",
        "    Conv2D(filters = 16, kernel_size = 3, padding = 'valid', strides = 1, activation = 'relu', kernel_regularizer = L2(regularizing_rate)),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size = 2, strides = 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(100, activation = 'relu', kernel_regularizer = L2(regularizing_rate)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate = dropout_rate),\n",
        "    Dense(10, activation = 'relu', kernel_regularizer = L2(regularizing_rate)),\n",
        "    BatchNormalization(),\n",
        "    Dense(1, activation = 'sigmoid'),\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PDl7VyoXbsQ"
      },
      "outputs": [],
      "source": [
        "func_input = Input(shape = (IM_SIZE, IM_SIZE, 3), name = 'Input Image')\n",
        "x = Conv2D(filters = 6, kernel_size = 3, padding = 'valid', strides = 1, activation = 'relu')(func_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size = 2, strides = 2)(x)\n",
        "x = Conv2D(filters = 16, kernel_size = 3, padding = 'valid', strides = 1, activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "output = MaxPool2D(pool_size = 2, strides = 2)(x)\n",
        "\n",
        "feature_extractor_model = Model(func_input, output, name = \"feature_extractor\")\n",
        "feature_extractor_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b7IMmIYbfkh"
      },
      "outputs": [],
      "source": [
        "feature_extractor_seq = tf.keras.Sequential([\n",
        "    InputLayer(input_shape=(IM_SIZE, IM_SIZE, 3)),\n",
        "    Conv2D(filters = 6, kernel_size = 3, padding = 'valid', strides = 1, activation = 'relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size = 2, strides = 2),\n",
        "\n",
        "    Conv2D(filters = 16, kernel_size = 3, padding = 'valid', strides = 1, activation = 'relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size = 2, strides = 2),\n",
        "])\n",
        "feature_extractor_seq.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6htG-CJgaTzT"
      },
      "outputs": [],
      "source": [
        "func_input = Input(shape = (IM_SIZE, IM_SIZE, 3), name = 'Input Image')\n",
        "# x = feature_extractor_model(func_input)\n",
        "x = feature_extractor_seq(func_input)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(100, activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(10, activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "func_output = Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "\n",
        "lenet_model_func = Model(func_input, func_output, name = \"lenet_Model\")\n",
        "lenet_model_func.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9SBH-Trb0x1"
      },
      "source": [
        "# Model Subclassing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKCDzaPub5M3"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(Layer):\n",
        "  def __init__(self, filters, kernel_size, padding, strides, activation, pool_size):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.conv1 = Conv2D(filters = filters , kernel_size = kernel_size, padding = padding, strides = strides, activation = activation)\n",
        "    self.bn1 = BatchNormalization()\n",
        "    self.maxpool1 = MaxPool2D(pool_size = pool_size, strides = 2*strides)\n",
        "\n",
        "    self.conv2 = Conv2D(filters = 2*filters , kernel_size = kernel_size, padding = padding, strides = strides, activation = activation)\n",
        "    self.bn2 = BatchNormalization()\n",
        "    self.maxpool2 = MaxPool2D(pool_size = pool_size, strides = 2*strides)\n",
        "\n",
        "  def call(self, x, training):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.maxpool2(x)\n",
        "    return x\n",
        "\n",
        "feature_sub_classed = FeatureExtractor(8, 3, 'valid', 1, 'relu', 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtCwqz2eeu0w"
      },
      "outputs": [],
      "source": [
        "func_input = Input(shape = (IM_SIZE, IM_SIZE, 3), name = 'Input Image')\n",
        "x = feature_sub_classed(func_input)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(100, activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(10, activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "func_output = Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "\n",
        "lenet_model_func = Model(func_input, func_output, name = \"lenet_Model\")\n",
        "lenet_model_func.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNJcQQCvfNnX"
      },
      "outputs": [],
      "source": [
        "class LenetModel(Model):\n",
        "  def __init__(self):\n",
        "    super(LenetModel, self).__init__()\n",
        "    self.feature_extractor = FeatureExtractor(8, 3, 'valid', 1, 'relu', 2)\n",
        "\n",
        "    self.flatten = Flatten()\n",
        "    self.dense1 = Dense(100, activation = 'relu')\n",
        "\n",
        "    self.bn1 = BatchNormalization()\n",
        "    self.dense2 = Dense(10, activation = 'relu')\n",
        "    self.bn2 = BatchNormalization()\n",
        "\n",
        "    self.dense3 = Dense(1, activation = 'sigmoid')\n",
        "\n",
        "  def call(self, x, training):\n",
        "    x = self.feature_extractor(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.dense1(x)\n",
        "    x = self.bn1(x)\n",
        "\n",
        "    x = self.dense2(x)\n",
        "    x = self.bn2(x)\n",
        "\n",
        "    x = self.dense3(x)\n",
        "    return x\n",
        "\n",
        "lenet_subclassed_model = LenetModel()\n",
        "lenet_subclassed_model(tf.zeros([1, 224, 224, 3]), training = False)\n",
        "lenet_subclassed_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttc_tGoTIoCF"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENnAak2CIqmZ"
      },
      "outputs": [],
      "source": [
        "class LossCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        print(f'\\n for epoch number {epoch} the model has loss of {logs[\"loss\"]}')\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        print(f'\\n for batch number {batch} the model has loss of {logs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7vFXvoNFyD"
      },
      "source": [
        "## CSV Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BvVuAagLOqy"
      },
      "outputs": [],
      "source": [
        "csv_callback = CSVLogger(\n",
        "    'logs.csv',\n",
        "    separator = ',',\n",
        "    append = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxstVg-nNIFz"
      },
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1gTIXqpNKRi"
      },
      "outputs": [],
      "source": [
        "es_callback = EarlyStopping(\n",
        "    monitor='val_loss', min_delta = 0, patience = 2, verbose = 1, mode = 'auto', baseline = None, restore_best_weights = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvb4026EQutd"
      },
      "source": [
        "## Learning Rate Schedular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnovCOKcQxtR"
      },
      "outputs": [],
      "source": [
        "def scheduler(epoch, lr):\n",
        "    if epoch < 3:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "scheduler_callack = LearningRateScheduler(scheduler, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp_B5TmpTWPg"
      },
      "source": [
        "## Model Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huHYLIizTZFr"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    'ceheckpoints/', mode = 'auto', monitor = 'val_loss', save_best_only = True, save_weights_only = False, save_freq = 'epoch', verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfGfktrAIkUM"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSWrFtxRCsoM"
      },
      "outputs": [],
      "source": [
        "metrics = [TruePositives(name= 'tp'), FalsePositives(name = 'fp'), TrueNegatives(name = 'tn'), FalseNegatives(name = 'fn'),\n",
        "           BinaryAccuracy(name = 'accuracy'), Precision(name = 'precision'), Recall(name = 'recall'), AUC(name = 'auc')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apgDR9KS1q3M"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer = Adam(learning_rate = 0.01),\n",
        "    loss = BinaryCrossentropy(),\n",
        "    metrics = metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjGAhyXG4r7M"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=5, verbose=1, callbacks = [scheduler_callack, checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpWloVkd5oWu"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val_loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yNfeMuQIyuE"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('rmse')\n",
        "plt.legend(['train', 'val performance'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeK2mxyzJKdo"
      },
      "source": [
        "# Model Evaluation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhTZnVgpI2ZN"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVAQ-_agJfnI"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkdayPocJ3bD"
      },
      "outputs": [],
      "source": [
        "test_dataset = test_dataset.batch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twJ5rEk3J65I"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVmXme1bKE16"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXJJHxtXLDZG"
      },
      "outputs": [],
      "source": [
        "model.predict(test_dataset.take(1))[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDBh-qJJE5Rm"
      },
      "source": [
        "# Visualizing Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fLN8YXWE469"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "inp = []\n",
        "for x,y in test_dataset.as_numpy_iterator():\n",
        "    labels.append(y)\n",
        "    inp.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSvCm22aGJp2"
      },
      "outputs": [],
      "source": [
        "print(np.array(inp).shape)\n",
        "print(np.array(inp)[:,0, ...].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4HXXVx6FWRV"
      },
      "outputs": [],
      "source": [
        "labels = np.array([i[0] for i in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utTvlsfcFYkM"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqAWy5vsF5lR"
      },
      "outputs": [],
      "source": [
        "predicted = model.predict(np.array(inp)[:,0, ...])\n",
        "predicted[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhXezBjrGrxB"
      },
      "outputs": [],
      "source": [
        "threshold = 0.41\n",
        "\n",
        "cm = confusion_matrix(labels, predicted > threshold)\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize = (8,8))\n",
        "\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.title('confusion matrix - {}'.format(threshold))\n",
        "plt.ylabel('actual')\n",
        "plt.xlabel('predicted')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNQ_n8IOH-JT"
      },
      "source": [
        "# ROC plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C01rCvekIAbA"
      },
      "outputs": [],
      "source": [
        "fp, tp, thresholds = roc_curve(labels, predicted)\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.plot(fp, tp)\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('true positive rate')\n",
        "plt.grid()\n",
        "\n",
        "skip = 20\n",
        "for i in range(0, len(thresholds), skip):\n",
        "    plt.text(fp[i], tp[i], thresholds[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbaZZSGyLbbM"
      },
      "outputs": [],
      "source": [
        "def parasite_or_not(x):\n",
        "  if(x < 0.5):\n",
        "    return 'P'\n",
        "  else:\n",
        "    return 'U'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wrjIQQoLrzl"
      },
      "outputs": [],
      "source": [
        "parasite_or_not(model.predict(test_dataset.take(1))[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cvYfCUbL0N0"
      },
      "outputs": [],
      "source": [
        "for i , (image, label) in enumerate(test_dataset.take(16)):\n",
        "  ax = plt.subplot(4, 4, i+1)\n",
        "  plt.imshow(image[0])\n",
        "  plt.title(str(parasite_or_not(label.numpy()[0])) + ':' + str(parasite_or_not(model.predict(image)[0][0])))\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pj42fk0WPWv"
      },
      "outputs": [],
      "source": [
        "# model.save('malaria_diagnosis.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaNpxfSzBaRT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cCbLOutnnoKB",
        "fmtspfgYnw63",
        "tzPQ3aVV_RIF",
        "N9SBH-Trb0x1"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}